{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9d9956d",
   "metadata": {},
   "source": [
    "# Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "id": "32024fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f93733d4",
   "metadata": {},
   "source": [
    "### Preprocessing The Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "id": "31a5a7b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "f1        object\n",
       "f2        object\n",
       "f3         int64\n",
       "f4        object\n",
       "f5        object\n",
       "f6        object\n",
       "f7        object\n",
       "target     int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 447,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Read the set\n",
    "df = pd.read_csv('dataset.csv')\n",
    "df.head()\n",
    "pass\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "4d2fa3eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Deleting rows with alphabets\n",
    "for i in df.columns:\n",
    "    if df[i].dtype == object:\n",
    "        df = df[~df[i].str.match(r'[^\\d.]+')]\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "# save the preprocessed data to a new CSV file\n",
    "df.to_csv(\"preprocessed_data.csv\", index=False)\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "e6a88aeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "f1          int64\n",
       "f2          int64\n",
       "f3          int64\n",
       "f4          int64\n",
       "f5          int64\n",
       "f6        float64\n",
       "f7        float64\n",
       "target      int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Now we can check and confirm that all our columns are numerical \n",
    "preprocessed = pd.read_csv('preprocessed_data.csv')\n",
    "preprocessed.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "67b6c51d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scaling the dataset so lets get the target out\n",
    "target = preprocessed.pop('target')\n",
    "#Take the numerical cols\n",
    "numerical_cols = ['f1', 'f2', 'f3', 'f4', 'f5', 'f6', 'f7']\n",
    "preprocessed[numerical_cols] = torch.nn.functional.normalize(torch.tensor(preprocessed[numerical_cols].values), dim=0).numpy()\n",
    "#Add back target\n",
    "preprocessed['target'] = target\n",
    "preprocessed.to_csv(\"scaled_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "f69205a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "f1        float64\n",
       "f2        float64\n",
       "f3        float64\n",
       "f4        float64\n",
       "f5        float64\n",
       "f6        float64\n",
       "f7        float64\n",
       "target      int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('scaled_data.csv')\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "e43b5712",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (608, 7), y_train shape: (608,)\n",
      "X_val shape: (76, 7), y_val shape: (76,)\n",
      "X_test shape: (76, 7), y_test shape: (76,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# Split the data into features (X) and target (y)\n",
    "X = df.drop(columns=['target'])\n",
    "y = df['target']\n",
    "\n",
    "# Split the data into train and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Split the validation set into validation and test sets\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_val, y_val, test_size=0.5, random_state=42)\n",
    "\n",
    "# Print the shapes of the resulting arrays\n",
    "print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
    "print(f\"X_val shape: {X_val.shape}, y_val shape: {y_val.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "b3f3d5b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transforming our dataframes to tensors\n",
    "X_train = torch.Tensor(X_train.values)\n",
    "X_val = torch.Tensor(X_val.values)\n",
    "X_test = torch.Tensor(X_test.values)\n",
    "y_train = torch.LongTensor(y_train.values)\n",
    "y_val = torch.LongTensor(y_val.values)\n",
    "y_test = torch.LongTensor(y_test.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96092e48",
   "metadata": {},
   "source": [
    "### Implementing Our Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "da3c2c1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNet(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=7, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=128, out_features=2, bias=True)\n",
      "    (7): Sigmoid()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "#Defining our Neural Network according to given structure with 3 hidden layers, 128 nodes per layer and 7 input neurons for 7 features\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(7, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 2),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "\n",
    "# Create an instance of the neural network\n",
    "model = NeuralNet()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "eda94036",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " For Epoch [10/100], training loss is: 0.513 and validation loss is: 0.525\n",
      " For Epoch [20/100], training loss is: 0.510 and validation loss is: 0.520\n",
      " For Epoch [30/100], training loss is: 0.513 and validation loss is: 0.525\n",
      " For Epoch [40/100], training loss is: 0.515 and validation loss is: 0.525\n",
      " For Epoch [50/100], training loss is: 0.509 and validation loss is: 0.520\n",
      " For Epoch [60/100], training loss is: 0.512 and validation loss is: 0.544\n",
      " For Epoch [70/100], training loss is: 0.520 and validation loss is: 0.528\n",
      " For Epoch [80/100], training loss is: 0.509 and validation loss is: 0.529\n",
      " For Epoch [90/100], training loss is: 0.507 and validation loss is: 0.536\n",
      " For Epoch [100/100], training loss is: 0.506 and validation loss is: 0.509\n",
      "Accuracy of the model on the test set is: 76.32%\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Define the batch size\n",
    "batch_size = 32\n",
    "#Dataloader\n",
    "train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Create a DataLoader for the validation set\n",
    "val_loader = DataLoader(TensorDataset(X_val, y_val), batch_size=batch_size)\n",
    "\n",
    "# Defining the loss function and optimizer\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "# Training the model\n",
    "#Lets define this as our fit function\n",
    "def fit():\n",
    "    num_epochs = 100\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training\n",
    "        train_loss = 0\n",
    "        for (inputs, targets) in train_loader:\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_fn(outputs, targets)\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Calculate training loss\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        # Validation\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for (inputs, targets) in val_loader:\n",
    "                outputs = model(inputs)\n",
    "                loss = loss_fn(outputs, targets)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        # Print epoch losses for every 10 epochs\n",
    "        if (epoch+1)%10 == 0:\n",
    "            print(f\" For Epoch [{epoch+1}/{num_epochs}], training loss is: {train_loss/len(train_loader):.3f} and validation loss is: {val_loss/len(val_loader):.3f}\")\n",
    "fit()\n",
    "#Now make predictions on test set\n",
    "with torch.no_grad():\n",
    "    outputs = model(X_test)\n",
    "    predicted = torch.argmax(outputs, dim=1)\n",
    "    accuracy = (predicted == y_test).sum().item() / len(y_test)\n",
    "    print(f\"Accuracy of the model on the test set is: {accuracy*100:.2f}%\")\n",
    "torch.save(model.state_dict(), 'rahuladi_gurukavy_assignment2_part1.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd9809a",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "39be363c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix:\n",
      "[[44  8]\n",
      " [10 14]]\n"
     ]
    }
   ],
   "source": [
    "#Visualization with confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Get the predictions on the validation set\n",
    "with torch.no_grad():\n",
    "    outputs = model(X_val)\n",
    "    predicted = torch.argmax(outputs, dim=1)\n",
    "\n",
    "# Compute the confusion matrix\n",
    "cm = confusion_matrix(y_val, predicted)\n",
    "\n",
    "# Print the confusion matrix\n",
    "print('Confusion matrix:')\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "fe2b0f10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWgAAAEGCAYAAABIGw//AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAUyElEQVR4nO3df7ScVX3v8ffnHBBQfiVAYgQV1xWVnwaqCCKIgBpAJShWaEWu4g32XihURSkqghRLl1bau9qKEZBoEaGCgICFNJaCyBUQQgjkWrouiGBMJPwIIqAJ+/4xD/EknJyZk8yc2Sd5v9Z61pnZM8+e78nK+mRnP/vZk1IKkqT6DPS7AEnS8AxoSaqUAS1JlTKgJalSBrQkVWqDfhewOh/L5i4v0Quc+9Qv+l2CavTiLbK2XYwmc84tS9f68zpRbUBL0liqcTrBgJYkYCBjMigeFQNaknAELUnVGqhvAG1ASxLABk5xSFKdnOKQpEo5xSFJlXIELUmVinPQklQnR9CSVKkN6htAG9CSBN5JKEnVcopDkirlMjtJqpQjaEmqlLd6S1KlHEFLUqWcg5akSg1QX0Ib0JJEnSPoGqddJGnMDYzi6ESSwSR3Jrm6eT4xyewk9zU/J3RSkySt9zZIOj46dCKwYMjzU4A5pZQdgDnN8xEZ0JJEa4qj06OdJNsBhwLnDWk+DJjVPJ4FTG9b06h/C0laB2UURwf+DvgU8NyQtsmllIUAzc9J7ToxoCWJ0Y2gk8xIcvuQY8bz/SR5F7C4lPLTta3JVRySxOiW2ZVSZgIzV/PyPsB7khwCbAxsnuSfgUVJppRSFiaZAixuX5MkqWtz0KWUvyylbFdK2R44EvhhKeWDwFXAMc3bjgGubFeTI2hJAgZ7/xFnA5cmORZ4EHh/uxMMaEmiNxv2l1JuAG5oHi8BDhzN+Qa0JNHx6owxZUBLEga0JFXLgJakSvmlsZJUqRrXHBvQkgRUOIA2oCUJIBXOQhvQkoQXCSWpWga0JFVqsMJJaANaknAELUnVqnAAbUBLEjiClqRqjWbD/rFiQEsSjqAlqVqdfFv3WDOgJQnvJJSkajmClqRKVZjPBrQkgQEtSdWqccP+GveoXu9lYIBT77iJ//n9S1dqf/snTuDcspSXbDWxT5WpFhf+87c59H0f4F1HHMnHT/kszz77bL9LGvcGRnGMZU2qzAEn/hm/WvCfK7VN2G5bXvf2A1jy8wf7VJVqsWjxYr558SVcdtEsrv7ud1j+3HKuuW52v8sa9zKKY8R+ko2T3JrkriT3JDmjaT89ycNJ5jbHIe1qMqArs+W2L2PXQ9/JzefNWqn9/ef8NZd/6nNQSp8qU02WL1/OM88+y7Jly3jmmWeYtM3W/S5p3EvS8dHGs8ABpZTXA1OBaUn2al47p5QytTmubddRz+agk7wOOAzYFijAL4GrSikLevWZ64I//ruzufxTp7HxZpuuaNvt3Qfz+MMLeXje/D5WplpMnjSJj3zog7zt4Pew0UYbsc/eb+Ite+/V/kSNqFsz0KWUAvymebphc6zRyKonI+gknwa+Q+t3vhW4rXl8cZJTRjhvRpLbk9x+L7/rRWlV2/XQaTy5+BEevGPuirYNN9mEgz9zMleddlb/ClNVnli6lDk3/Adzrr6Cm66/lqeffporr/lBv8sa90YzxTE0q5pjxkp9JYNJ5gKLgdmllJ80Lx2fZF6SC5JMaFtT6cF/mZP8J7BzKeX3q7S/CLinlLJDuz4+ls3Xu//LT//i53nT0Ufy3LJlbLDxxmyy+WbM/8FsXr3vm/n9b38LwJbbbcsTv1zI2Xu+jaWLFve54rF37lO/6HcJffeD2f/GTTffwhdP/xwAV3z/GubePZ/TT/10nyvroxdvsdYD4Du3fWXHmbP7wz/v6POSbAl8DzgB+DXwCK3R9JnAlFLKR0Y6v1dTHM8BLwN+vkr7lOY1DeOKU8/gilPPAOA1b30LB33yz5l5xNErvees++/mi294K08tebQfJaoCL3vpS7nr7vk8/fQzbLzxRtxy623sstOO/S5r3EsPbiUspTye5AZgWinlyys+K/k6cHW783sV0CcBc5LcBzw/5HkF8Grg+B59prReeP2uu/DOgw7k8D85mg0GB9nxda/lA+87vN9ljXvdWgadZBvg9004bwIcBPxNkimllIXN2w4H2l5U6skUR1PkALAnrYuEAR4CbiulLO/k/PVxikPtOcWhYXVhimPeK7fvOHN2+/kDq/28JLsBs4BBWtf5Li2lfCHJt2it6ijAA8BxQwJ7WD1bxVFKeQ74P73qX5K6qYPlcx0ppcwDdh+m/ehh3j4ib/WWJPxOQkmq1kCF+40a0JJEnZslGdCShFMcklStbl0k7CYDWpKAVLh1nAEtSXiRUJKq5RSHJFWqwnw2oCUJXGYnSdWqMJ8NaEkC56AlqVoDLrOTpDr1YsP+tWVASxLOQUtStVzFIUmVqjCfDWhJAldxSFK13ItDkipV4QDagJYkqHOKo8Kl2ZI09jLQ+TFiP8nGSW5NcleSe5Kc0bRPTDI7yX3NzwntajKgJYnWCLrTo41ngQNKKa8HpgLTkuwFnALMKaXsAMxpno/IgJYkgMGBzo8RlJbfNE83bI4CHAbMatpnAdPblWRASxKjG0EnmZHk9iHHjFX6GkwyF1gMzC6l/ASYXEpZCND8nNSuJi8SShLAKJbZlVJmAjNHeH05MDXJlsD3kuyyRiWtyUmStM5JOj86VEp5HLgBmAYsSjKl9VGZQmt0PSIDWpJo7WbX6TFiP8k2zciZJJsABwH/F7gKOKZ52zHAle1qcopDkqCbd6pMAWYlGaQ1CL60lHJ1kluAS5McCzwIvL9dRwa0JAFpszqjU6WUecDuw7QvAQ4cTV8GtCTBqC4SjhUDWpKo81ZvA1qSwBG0JFXLEbQk1SmDBrQkVclv9ZakWjnFIUmVcgQtSXWqcZld21tn0vLBJKc1z1+RZM/elyZJY2ggnR9jVVIH7/knYG/gqOb5k8A/9qwiSeqDDAx0fIyVTqY43lRK2SPJnQCllMeSvKjHdUnS2Bqnc9C/b3ZlKtDaSg94rqdVSdIYq3EOupOA/t/A94BJSc4CjgA+29OqJGmsjccRdCnloiQ/pbVNXoDppZQFPa9MksbSeBxBJ3kF8Fvg+0PbSikP9rIwSRpL4/VOwmtozT8H2Bh4FfAzYOce1iVJY6tLG/Z3UydTHLsOfZ5kD+C4nlUkSX0wXi8SrqSUckeSN/aimKHOfeTeXn+ExqHn7p/X7xJUoYGd9+1CJ+MwoJN8fMjTAWAP4Nc9q0iS+mGcjqA3G/J4Ga056ct6U44k9cl4C+jmBpVNSyknj1E9ktQfg4Nd6SbJy4FvAi+ldVPfzFLK3yc5Hfgf/GEG4tRSyrUj9bXagE6yQSllWXNRUJLWbd0bQS8DPtFcr9sM+GmS2c1r55RSvtxpRyONoG+lNd88N8lVwL8ATz3/Yinl8tHXLUmV6lJAl1IWAgubx08mWQBsuyZ9dbLwbyKwBDgAeBfw7uanJK07ko6PJDOS3D7kmDF8l9ke2B34SdN0fJJ5SS5IMqFdSSONoCc1Kzjm84cbVZ5XOvqFJWm8GMU2oqWUmcDMkd6TZFNaCypOKqUsTfJV4Exa+Xkm8LfAR0bqY6SAHgQ2ZeVgXlHfSJ1K0rjTxVUcSTakFc4XPT8dXEpZNOT1rwNXt+tnpIBeWEr5wtoWKknjQpc24k/rlsTzgQWllK8MaZ/SzE8DHE5rdmJEIwV0fYsCJalXuvdNKfsARwN3J5nbtJ0KHJVkKq0ZiAfoYMuMkQL6wLUqUZLGk+6t4vgRww9wR1zzPJzVBnQp5dHRdiZJ49Z4u5NQktYbBrQk1Wksv627Uwa0JEE3LxJ2jQEtSeAUhyRVyxG0JFXKEbQkVcqAlqRKdWnD/m4yoCUJHEFLUrUMaEmqlKs4JKlSjqAlqVIGtCRVylUcklQpR9CSVCkDWpIqFVdxSFKdBhxBS1KdHEFLUqUqXMVR3z8ZktQPSefHiN3k5Un+PcmCJPckObFpn5hkdpL7mp8T2pVkQEsStKY4Oj1Gtgz4RCllR2Av4H8l2Qk4BZhTStkBmNM8H5EBLUnQtRF0KWVhKeWO5vGTwAJgW+AwYFbztlnA9HYlGdCSBK3Nkjo8ksxIcvuQY8ZwXSbZHtgd+AkwuZSyEFohDkxqV5IXCSUJYKDzi4SllJnAzJHek2RT4DLgpFLK0qzBjTAGtCRBV9dBJ9mQVjhfVEq5vGlelGRKKWVhkinA4rYlda0iSRrPunSRMK2h8vnAglLKV4a8dBVwTPP4GODKdiU5gpYk6OZeHPsARwN3J5nbtJ0KnA1cmuRY4EHg/e06MqAlCbp2J2Ep5UfA6tL+wNH0ZUBLErgXhyRVaxSrOMaKAS1J4AhakqrlbnaSVCm/UUWSKuUIWpIqVeF+0Aa0JEGVUxz1jenXc3951pfY+5D38a4/PXZF2+NLl/LhE0/mHX/8IT584sk8sfTJPlaofvjMP3yDff77X/DuE097wWsXXHEdO773ozzm34u1M4rd7MaspDH7JHXkvYe8k/PO+euV2mZ+62L2/qM9uP7Sb7L3H+3BzG9d3Kfq1C/T37YPMz930gvaFz7yKD+edy9Ttp449kWta7q0H3Q3GdCVeePuu7HF5puv1Dbnph8z/ZB3ADD9kHfwbzfd3I/S1Edv3Pk1bLnZS17QfvYFl/DJo49gTbay1Cq6940qXeMc9Diw5NHHmLT1VgBM2norHn3s8f4WpCr88Na5TN5qS173qpf3u5R1Q4X/yI35CDrJh0d4bcW3FMycddFYliWNK08/+yxfu+waTjjysH6Xsu4YHOz8GCP9GEGfAXxjuBdW+paCJQ+VMaypaltNnMDiR5YwaeutWPzIEiZO2LLfJanPfvGrX/PQokeY/vEzAFi05DHe98kzueRvPsM2E7boc3Xj1PqyDjrJvNW9BEzuxWeuyw54y5u54trrmfGho7ji2us5cN8397sk9dlrXrkdN194zornBx73ab77pc8yYfPN+ljVOFfhFEevRtCTgXcCj63SHuDHPfrMdcLHT/srbr3zLh57/An2O+wDnPDRY5hx9JGc9Nkz+e7VP2DK5En8/VkvXGqlddsnvjKTW+f/jMef/A37f/Rkjj/yPRxx0L79LmvdUuEIOqV0fyYhyfnAN5qNq1d97dullD9p24lTHBrGc7+6v98lqEIDO++71sPf5Tde0nHmDO73gTEZbvdkBF1KOXaE19qHsySNtQpH0C6zkyRww35JqlWNN/sY0JIEVU5x1FeRJPVDF2/1TnJBksVJ5g9pOz3Jw0nmNsch7foxoCUJWt9J2OnR3oXAtGHazymlTG2Oa9t14hSHJEFXLxKWUm5Msv3a9uMIWpJgrHazOz7JvGYKZEK7NxvQkgSj2g966MZuzTGjg0/4KvDfgKnAQuBv253gFIckwahGxitt7Nb5OYtWfFTydeDqducY0JIEnV78W2NJppRSFjZPDwfmj/R+MKAlqaWL66CTXAzsD2yd5CHg88D+SaYCBXgAOK5dPwa0JEG3V3EcNUzz+aPtx4CWJFiv9oOWpPGlwlu9DWhJAhgwoCWpSu5mJ0m1copDkirlhv2SVCmnOCSpUl4klKRKOYKWpEp5kVCSKmVAS1KlnOKQpEoZ0JJUKwNakurkCFqSKlVfPhvQkgS4ikOSquUUhyTVyoCWpDo5gpakWhnQklSnCkfQ9V22lKR+yEDnR7uukguSLE4yf0jbxCSzk9zX/JzQrh8DWpJofSdhp0cHLgSmrdJ2CjCnlLIDMKd5PiIDWpKgNcXR6dFGKeVG4NFVmg8DZjWPZwHT2/VjQEsS0LpI2NmRZEaS24ccMzr4gMmllIUAzc9J7U7wIqEkwaguEpZSZgIze1dMiwEtSTAWt3ovSjKllLIwyRRgcbsTnOKQJOjqHPRqXAUc0zw+Briy3QkGtCTBaKag23eVXAzcArw2yUNJjgXOBt6e5D7g7c3zETnFIUlAN+8kLKUctZqXDhxNPwa0JEGVdxIa0JIEBrQkVcsN+yWpUo6gJalWBrQk1anCEXRKKf2uQW0kmdHcWiqt4N+LdV99s+IaTicbsWj949+LdZwBLUmVMqAlqVIG9PjgPKOG49+LdZwXCSWpUo6gJalSBrQkVcqArlySaUl+luS/krT9FmCt+5JckGRxkvn9rkW9ZUBXLMkg8I/AwcBOwFFJdupvVarAhcC0fheh3jOg67Yn8F+llP9XSvkd8B1aX92u9Vgp5Ubg0X7Xod4zoOu2LfCLIc8fatokrQcM6LoNt3uL6yKl9YQBXbeHgJcPeb4d8Ms+1SJpjBnQdbsN2CHJq5K8CDiS1le3S1oPGNAVK6UsA44HrgMWAJeWUu7pb1XqtyQXA7cAr03yUJJj+12TesNbvSWpUo6gJalSBrQkVcqAlqRKGdCSVCkDWpIqZUCrJ5IsTzI3yfwk/5LkxWvR14VJjmgenzfShlFJ9k/y5jX4jAeSbL2mNUq9YECrV54upUwtpewC/A742NAXm536Rq2U8tFSyr0jvGV/YNQBLdXIgNZYuAl4dTO6/fck3wbuTjKY5EtJbksyL8lxAGn5hyT3JrkGmPR8R0luSPKG5vG0JHckuSvJnCTb0/qH4C+a0fu+SbZJclnzGbcl2ac5d6sk1ye5M8nXGH7fE6mvNuh3AVq3JdmA1n7W/9o07QnsUkq5P8kM4IlSyhuTbATcnOR6YHfgtcCuwGTgXuCCVfrdBvg6sF/T18RSyqNJzgV+U0r5cvO+bwPnlFJ+lOQVtO7K3BH4PPCjUsoXkhwKzOjpH4S0Bgxo9comSeY2j28Czqc19XBrKeX+pv0dwG7Pzy8DWwA7APsBF5dSlgO/TPLDYfrfC7jx+b5KKavbH/kgYKdkxQB58ySbNZ/x3ubca5I8tma/ptQ7BrR65elSytShDU1IPjW0CTihlHLdKu87hPbbqqaD90BrGm/vUsrTw9TiPgeqmnPQ6qfrgD9LsiFAktckeQlwI3BkM0c9BXjbMOfeArw1yauacyc27U8Cmw153/W0Npyied/U5uGNwJ82bQcDE7r1S0ndYkCrn86jNb98R/MFqF+j9b+67wH3AXcDXwX+Y9UTSym/pjVvfHmSu4BLmpe+Dxz+/EVC4M+BNzQXIe/lD6tJzgD2S3IHramWB3v0O0przN3sJKlSjqAlqVIGtCRVyoCWpEoZ0JJUKQNakiplQEtSpQxoSarU/wdH9LaOR5xyNAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Visualization with heat map shows false negative, true negative, false positive and true positive\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Compute the confusion matrix\n",
    "cm = confusion_matrix(y_val, predicted)\n",
    "\n",
    "# Plot the confusion matrix as a heatmap\n",
    "sns.heatmap(cm, annot=True, cmap='Reds')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "9441d250",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Compute the confusion matrix\n",
    "cm = confusion_matrix(y_val, predicted)\n",
    "\n",
    "# Extract values for true positives, false positives, false negatives, and true negatives\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "# Create a bar chart\n",
    "labels = ['True Negative', 'False Positive', 'False Negative', 'True Positive']\n",
    "values = [tn, fp, fn, tp]\n",
    "colors = ['blue', 'orange', 'green', 'red']\n",
    "plt.bar(labels, values, color=colors)\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Prediction')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Confusion Matrix')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e664b2",
   "metadata": {},
   "source": [
    "# Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d0199c8",
   "metadata": {},
   "source": [
    "### Setup 1: Tuning Hyper Parameter-1 : Dropout with a dropout rate of 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "8d4e767f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNet(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=7, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.2, inplace=False)\n",
      "    (3): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (4): ReLU()\n",
      "    (5): Dropout(p=0.2, inplace=False)\n",
      "    (6): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Dropout(p=0.2, inplace=False)\n",
      "    (9): Linear(in_features=128, out_features=2, bias=True)\n",
      "    (10): Sigmoid()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#Now we will be adding dropout layers\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, dropout_rate):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(7, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=dropout_rate),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=dropout_rate),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=dropout_rate),\n",
    "            nn.Linear(128, 2),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "\n",
    "# Create an instance of the neural network\n",
    "model = NeuralNet(dropout_rate = 0.2)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "d245352b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " For Epoch [10/100], training loss is: 0.641 and validation loss is: 0.600\n",
      " For Epoch [20/100], training loss is: 0.610 and validation loss is: 0.547\n",
      " For Epoch [30/100], training loss is: 0.580 and validation loss is: 0.557\n",
      " For Epoch [40/100], training loss is: 0.564 and validation loss is: 0.547\n",
      " For Epoch [50/100], training loss is: 0.549 and validation loss is: 0.508\n",
      " For Epoch [60/100], training loss is: 0.544 and validation loss is: 0.496\n",
      " For Epoch [70/100], training loss is: 0.528 and validation loss is: 0.511\n",
      " For Epoch [80/100], training loss is: 0.523 and validation loss is: 0.471\n",
      " For Epoch [90/100], training loss is: 0.521 and validation loss is: 0.535\n",
      " For Epoch [100/100], training loss is: 0.515 and validation loss is: 0.495\n",
      "Accuracy of the model with dropout rate 0.2 is: 78.95%\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Define the batch size and create a DataLoader for the training set\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Create a DataLoader for the validation set\n",
    "val_loader = DataLoader(TensorDataset(X_val, y_val), batch_size=batch_size)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "# Train the model\n",
    "#Lets define this as our fit function\n",
    "def fit():\n",
    "    num_epochs = 100\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training\n",
    "        train_loss = 0\n",
    "        for (inputs, targets) in train_loader:\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_fn(outputs, targets)\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Calculate training loss\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        # Validation\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for (inputs, targets) in val_loader:\n",
    "                outputs = model(inputs)\n",
    "                loss = loss_fn(outputs, targets)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        # Print epoch losses for every 10 epochs\n",
    "        if (epoch+1)%10 == 0:\n",
    "            print(f\" For Epoch [{epoch+1}/{num_epochs}], training loss is: {train_loss/len(train_loader):.3f} and validation loss is: {val_loss/len(val_loader):.3f}\")\n",
    "fit()\n",
    "#Now make predictions on test set\n",
    "with torch.no_grad():\n",
    "    outputs = model(X_test)\n",
    "    predicted = torch.argmax(outputs, dim=1)\n",
    "    accuracy = (predicted == y_test).sum().item() / len(y_test)\n",
    "    print(f\"Accuracy of the model with dropout rate 0.2 is: {accuracy*100:.2f}%\")\n",
    "torch.save(model.state_dict(), 'rahuladi_gurukavy_assignment2_part2.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "976d7ee4",
   "metadata": {},
   "source": [
    "### Setup 2: Tuning Hyper Parameter-1 : Dropout with a dropout rate of 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "f0c5090d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNet(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=7, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.5, inplace=False)\n",
      "    (3): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (4): ReLU()\n",
      "    (5): Dropout(p=0.5, inplace=False)\n",
      "    (6): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Dropout(p=0.5, inplace=False)\n",
      "    (9): Linear(in_features=128, out_features=2, bias=True)\n",
      "    (10): Sigmoid()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Create an instance of the neural network\n",
    "model = NeuralNet(dropout_rate = 0.5)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "2b937bf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " For Epoch [10/100], training loss is: 0.651 and validation loss is: 0.614\n",
      " For Epoch [20/100], training loss is: 0.644 and validation loss is: 0.601\n",
      " For Epoch [30/100], training loss is: 0.616 and validation loss is: 0.567\n",
      " For Epoch [40/100], training loss is: 0.592 and validation loss is: 0.536\n",
      " For Epoch [50/100], training loss is: 0.586 and validation loss is: 0.527\n",
      " For Epoch [60/100], training loss is: 0.575 and validation loss is: 0.541\n",
      " For Epoch [70/100], training loss is: 0.558 and validation loss is: 0.515\n",
      " For Epoch [80/100], training loss is: 0.551 and validation loss is: 0.542\n",
      " For Epoch [90/100], training loss is: 0.542 and validation loss is: 0.517\n",
      " For Epoch [100/100], training loss is: 0.541 and validation loss is: 0.547\n",
      "Accuracy of the model with dropout rate 0.5 is: 78.95%\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Define the batch size and create a DataLoader for the training set\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Create a DataLoader for the validation set\n",
    "val_loader = DataLoader(TensorDataset(X_val, y_val), batch_size=batch_size)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "# Train the model\n",
    "#Lets define this as our fit function\n",
    "def fit():\n",
    "    num_epochs = 100\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training\n",
    "        train_loss = 0\n",
    "        for (inputs, targets) in train_loader:\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_fn(outputs, targets)\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Calculate training loss\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        # Validation\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for (inputs, targets) in val_loader:\n",
    "                outputs = model(inputs)\n",
    "                loss = loss_fn(outputs, targets)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        # Print epoch losses for every 10 epochs\n",
    "        if (epoch+1)%10 == 0:\n",
    "            print(f\" For Epoch [{epoch+1}/{num_epochs}], training loss is: {train_loss/len(train_loader):.3f} and validation loss is: {val_loss/len(val_loader):.3f}\")\n",
    "fit()\n",
    "#Now make predictions on test set\n",
    "with torch.no_grad():\n",
    "    outputs = model(X_test)\n",
    "    predicted = torch.argmax(outputs, dim=1)\n",
    "    accuracy = (predicted == y_test).sum().item() / len(y_test)\n",
    "    print(f\"Accuracy of the model with dropout rate 0.5 is: {accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956a99c7",
   "metadata": {},
   "source": [
    "### Setup 3: Tuning Hyper Parameter-1 : Dropout with a dropout rate of 0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "87779114",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNet(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=7, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.6, inplace=False)\n",
      "    (3): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (4): ReLU()\n",
      "    (5): Dropout(p=0.6, inplace=False)\n",
      "    (6): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Dropout(p=0.6, inplace=False)\n",
      "    (9): Linear(in_features=128, out_features=2, bias=True)\n",
      "    (10): Sigmoid()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Create an instance of the neural network\n",
    "model = NeuralNet(dropout_rate = 0.6)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "ebaf6afa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " For Epoch [10/100], training loss is: 0.647 and validation loss is: 0.588\n",
      " For Epoch [20/100], training loss is: 0.638 and validation loss is: 0.594\n",
      " For Epoch [30/100], training loss is: 0.626 and validation loss is: 0.567\n",
      " For Epoch [40/100], training loss is: 0.599 and validation loss is: 0.585\n",
      " For Epoch [50/100], training loss is: 0.599 and validation loss is: 0.558\n",
      " For Epoch [60/100], training loss is: 0.590 and validation loss is: 0.542\n",
      " For Epoch [70/100], training loss is: 0.561 and validation loss is: 0.531\n",
      " For Epoch [80/100], training loss is: 0.565 and validation loss is: 0.521\n",
      " For Epoch [90/100], training loss is: 0.550 and validation loss is: 0.530\n",
      " For Epoch [100/100], training loss is: 0.558 and validation loss is: 0.534\n",
      "Accuracy of the model with dropout rate 0.6 is: 67.11%\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Define the batch size and create a DataLoader for the training set\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Create a DataLoader for the validation set\n",
    "val_loader = DataLoader(TensorDataset(X_val, y_val), batch_size=batch_size)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "# Train the model\n",
    "#Lets define this as our fit function\n",
    "def fit():\n",
    "    num_epochs = 100\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training\n",
    "        train_loss = 0\n",
    "        for (inputs, targets) in train_loader:\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_fn(outputs, targets)\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Calculate training loss\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        # Validation\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for (inputs, targets) in val_loader:\n",
    "                outputs = model(inputs)\n",
    "                loss = loss_fn(outputs, targets)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        # Print epoch losses for every 10 epochs\n",
    "        if (epoch+1)%10 == 0:\n",
    "            print(f\" For Epoch [{epoch+1}/{num_epochs}], training loss is: {train_loss/len(train_loader):.3f} and validation loss is: {val_loss/len(val_loader):.3f}\")\n",
    "fit()\n",
    "#Now make predictions on test set\n",
    "with torch.no_grad():\n",
    "    outputs = model(X_test)\n",
    "    predicted = torch.argmax(outputs, dim=1)\n",
    "    accuracy = (predicted == y_test).sum().item() / len(y_test)\n",
    "    print(f\"Accuracy of the model with dropout rate 0.6 is: {accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b4aa6c",
   "metadata": {},
   "source": [
    "### Setup 4: Truning Hyper Parameter-2 : Using RMSprop Optimizer lets fix dropout at 0.2 as it gave best result previously"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "488afec1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNet(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=7, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.2, inplace=False)\n",
      "    (3): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (4): ReLU()\n",
      "    (5): Dropout(p=0.2, inplace=False)\n",
      "    (6): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Dropout(p=0.2, inplace=False)\n",
      "    (9): Linear(in_features=128, out_features=2, bias=True)\n",
      "    (10): Sigmoid()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Create an instance of the neural network\n",
    "model = NeuralNet(dropout_rate = 0.2)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "6c746811",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " For Epoch [10/100], training loss is: 0.643 and validation loss is: 0.590\n",
      " For Epoch [20/100], training loss is: 0.595 and validation loss is: 0.567\n",
      " For Epoch [30/100], training loss is: 0.586 and validation loss is: 0.557\n",
      " For Epoch [40/100], training loss is: 0.582 and validation loss is: 0.527\n",
      " For Epoch [50/100], training loss is: 0.562 and validation loss is: 0.531\n",
      " For Epoch [60/100], training loss is: 0.561 and validation loss is: 0.525\n",
      " For Epoch [70/100], training loss is: 0.537 and validation loss is: 0.508\n",
      " For Epoch [80/100], training loss is: 0.538 and validation loss is: 0.537\n",
      " For Epoch [90/100], training loss is: 0.537 and validation loss is: 0.540\n",
      " For Epoch [100/100], training loss is: 0.529 and validation loss is: 0.513\n",
      "Accuracy of the model with an RMSprop Optimizer is: 73.68%\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Define the batch size and create a DataLoader for the training set\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Create a DataLoader for the validation set\n",
    "val_loader = DataLoader(TensorDataset(X_val, y_val), batch_size=batch_size)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.RMSprop(model.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "# Train the model\n",
    "#Lets define this as our fit function\n",
    "def fit():\n",
    "    num_epochs = 100\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training\n",
    "        train_loss = 0\n",
    "        for (inputs, targets) in train_loader:\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_fn(outputs, targets)\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Calculate training loss\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        # Validation\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for (inputs, targets) in val_loader:\n",
    "                outputs = model(inputs)\n",
    "                loss = loss_fn(outputs, targets)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        # Print epoch losses for every 10 epochs\n",
    "        if (epoch+1)%10 == 0:\n",
    "            print(f\" For Epoch [{epoch+1}/{num_epochs}], training loss is: {train_loss/len(train_loader):.3f} and validation loss is: {val_loss/len(val_loader):.3f}\")\n",
    "fit()\n",
    "#Now make predictions on test set\n",
    "with torch.no_grad():\n",
    "    outputs = model(X_test)\n",
    "    predicted = torch.argmax(outputs, dim=1)\n",
    "    accuracy = (predicted == y_test).sum().item() / len(y_test)\n",
    "    print(f\"Accuracy of the model with an RMSprop Optimizer is: {accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c2b7ee",
   "metadata": {},
   "source": [
    "### Setup 5: Truning Hyper Parameter-2 : Using SGD Optimizer lets fix dropout at 0.2 as it yielded our best result previously"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "bc91745a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNet(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=7, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.2, inplace=False)\n",
      "    (3): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (4): ReLU()\n",
      "    (5): Dropout(p=0.2, inplace=False)\n",
      "    (6): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Dropout(p=0.2, inplace=False)\n",
      "    (9): Linear(in_features=128, out_features=2, bias=True)\n",
      "    (10): Sigmoid()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Create an instance of the neural network\n",
    "model = NeuralNet(dropout_rate = 0.2)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "40243518",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " For Epoch [10/100], training loss is: 0.689 and validation loss is: 0.687\n",
      " For Epoch [20/100], training loss is: 0.688 and validation loss is: 0.685\n",
      " For Epoch [30/100], training loss is: 0.687 and validation loss is: 0.684\n",
      " For Epoch [40/100], training loss is: 0.686 and validation loss is: 0.683\n",
      " For Epoch [50/100], training loss is: 0.685 and validation loss is: 0.682\n",
      " For Epoch [60/100], training loss is: 0.685 and validation loss is: 0.681\n",
      " For Epoch [70/100], training loss is: 0.684 and validation loss is: 0.680\n",
      " For Epoch [80/100], training loss is: 0.683 and validation loss is: 0.678\n",
      " For Epoch [90/100], training loss is: 0.683 and validation loss is: 0.678\n",
      " For Epoch [100/100], training loss is: 0.682 and validation loss is: 0.676\n",
      "Accuracy of the model with an SGD Optimizer is: 63.16%\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Define the batch size and create a DataLoader for the training set\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Create a DataLoader for the validation set\n",
    "val_loader = DataLoader(TensorDataset(X_val, y_val), batch_size=batch_size)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "# Train the model\n",
    "#Lets define this as our fit function\n",
    "def fit():\n",
    "    num_epochs = 100\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training\n",
    "        train_loss = 0\n",
    "        for (inputs, targets) in train_loader:\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_fn(outputs, targets)\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Calculate training loss\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        # Validation\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for (inputs, targets) in val_loader:\n",
    "                outputs = model(inputs)\n",
    "                loss = loss_fn(outputs, targets)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        # Print epoch losses for every 10 epochs\n",
    "        if (epoch+1)%10 == 0:\n",
    "            print(f\" For Epoch [{epoch+1}/{num_epochs}], training loss is: {train_loss/len(train_loader):.3f} and validation loss is: {val_loss/len(val_loader):.3f}\")\n",
    "fit()\n",
    "#Now make predictions on test set\n",
    "with torch.no_grad():\n",
    "    outputs = model(X_test)\n",
    "    predicted = torch.argmax(outputs, dim=1)\n",
    "    accuracy = (predicted == y_test).sum().item() / len(y_test)\n",
    "    print(f\"Accuracy of the model with an SGD Optimizer is: {accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baba2427",
   "metadata": {},
   "source": [
    "### Setup 6: Truning Hyper Parameter-2 : Using Adagrad Optimizer lets fix dropout at 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "008b4d3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNet(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=7, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.2, inplace=False)\n",
      "    (3): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (4): ReLU()\n",
      "    (5): Dropout(p=0.2, inplace=False)\n",
      "    (6): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Dropout(p=0.2, inplace=False)\n",
      "    (9): Linear(in_features=128, out_features=2, bias=True)\n",
      "    (10): Sigmoid()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Create an instance of the neural network\n",
    "model = NeuralNet(dropout_rate = 0.2)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "4710a2e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " For Epoch [10/100], training loss is: 0.649 and validation loss is: 0.610\n",
      " For Epoch [20/100], training loss is: 0.647 and validation loss is: 0.615\n",
      " For Epoch [30/100], training loss is: 0.649 and validation loss is: 0.606\n",
      " For Epoch [40/100], training loss is: 0.649 and validation loss is: 0.605\n",
      " For Epoch [50/100], training loss is: 0.648 and validation loss is: 0.606\n",
      " For Epoch [60/100], training loss is: 0.648 and validation loss is: 0.608\n",
      " For Epoch [70/100], training loss is: 0.647 and validation loss is: 0.601\n",
      " For Epoch [80/100], training loss is: 0.646 and validation loss is: 0.606\n",
      " For Epoch [90/100], training loss is: 0.646 and validation loss is: 0.609\n",
      " For Epoch [100/100], training loss is: 0.646 and validation loss is: 0.609\n",
      "Accuracy of the model with an Adagrad Optimizer is: 63.16%\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Define the batch size and create a DataLoader for the training set\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Create a DataLoader for the validation set\n",
    "val_loader = DataLoader(TensorDataset(X_val, y_val), batch_size=batch_size)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adagrad(model.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "# Train the model\n",
    "#Lets define this as our fit function\n",
    "def fit():\n",
    "    num_epochs = 100\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training\n",
    "        train_loss = 0\n",
    "        for (inputs, targets) in train_loader:\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_fn(outputs, targets)\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Calculate training loss\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        # Validation\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for (inputs, targets) in val_loader:\n",
    "                outputs = model(inputs)\n",
    "                loss = loss_fn(outputs, targets)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        # Print epoch losses for every 10 epochs\n",
    "        if (epoch+1)%10 == 0:\n",
    "            print(f\" For Epoch [{epoch+1}/{num_epochs}], training loss is: {train_loss/len(train_loader):.3f} and validation loss is: {val_loss/len(val_loader):.3f}\")\n",
    "fit()\n",
    "#Now make predictions on test set\n",
    "with torch.no_grad():\n",
    "    outputs = model(X_test)\n",
    "    predicted = torch.argmax(outputs, dim=1)\n",
    "    accuracy = (predicted == y_test).sum().item() / len(y_test)\n",
    "    print(f\"Accuracy of the model with an Adagrad Optimizer is: {accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "529c9db3",
   "metadata": {},
   "source": [
    "### Setup 7: Tuning Hyper Parameter-3 : Activation Function as Softmax lets keep dropout at 0.2 and fix Adam Optimizer as best results were obtained this way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "cc4418f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNet(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=7, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.2, inplace=False)\n",
      "    (3): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (4): ReLU()\n",
      "    (5): Dropout(p=0.2, inplace=False)\n",
      "    (6): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Dropout(p=0.2, inplace=False)\n",
      "    (9): Linear(in_features=128, out_features=2, bias=True)\n",
      "    (10): Softmax(dim=1)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#Now we will be adding dropout layers\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, dropout_rate):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(7, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=dropout_rate),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=dropout_rate),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=dropout_rate),\n",
    "            nn.Linear(128, 2),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "\n",
    "# Create an instance of the neural network\n",
    "model = NeuralNet(dropout_rate = 0.2)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "b063f9cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " For Epoch [10/100], training loss is: 0.511 and validation loss is: 0.513\n",
      " For Epoch [20/100], training loss is: 0.513 and validation loss is: 0.518\n",
      " For Epoch [30/100], training loss is: 0.504 and validation loss is: 0.513\n",
      " For Epoch [40/100], training loss is: 0.517 and validation loss is: 0.527\n",
      " For Epoch [50/100], training loss is: 0.518 and validation loss is: 0.515\n",
      " For Epoch [60/100], training loss is: 0.514 and validation loss is: 0.523\n",
      " For Epoch [70/100], training loss is: 0.512 and validation loss is: 0.538\n",
      " For Epoch [80/100], training loss is: 0.505 and validation loss is: 0.496\n",
      " For Epoch [90/100], training loss is: 0.512 and validation loss is: 0.515\n",
      " For Epoch [100/100], training loss is: 0.509 and validation loss is: 0.511\n",
      "Accuracy of the model on the test set is: 77.63%\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Define the batch size and create a DataLoader for the training set\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Create a DataLoader for the validation set\n",
    "val_loader = DataLoader(TensorDataset(X_val, y_val), batch_size=batch_size)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "# Train the model\n",
    "#Lets define this as our fit function\n",
    "def fit():\n",
    "    num_epochs = 100\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training\n",
    "        train_loss = 0\n",
    "        for (inputs, targets) in train_loader:\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_fn(outputs, targets)\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Calculate training loss\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        # Validation\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for (inputs, targets) in val_loader:\n",
    "                outputs = model(inputs)\n",
    "                loss = loss_fn(outputs, targets)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        # Print epoch losses for every 10 epochs\n",
    "        if (epoch+1)%10 == 0:\n",
    "            print(f\" For Epoch [{epoch+1}/{num_epochs}], training loss is: {train_loss/len(train_loader):.3f} and validation loss is: {val_loss/len(val_loader):.3f}\")\n",
    "fit()\n",
    "#Now make predictions on test set\n",
    "with torch.no_grad():\n",
    "    outputs = model(X_test)\n",
    "    predicted = torch.argmax(outputs, dim=1)\n",
    "    accuracy = (predicted == y_test).sum().item() / len(y_test)\n",
    "    print(f\"Accuracy of the model on the test set is: {accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba5b054",
   "metadata": {},
   "source": [
    "### Setup 8: Tuning Hyper Parameter-3 : Activation Function as Tanh lets keep dropout at 0.2 and fix Adam Optimizer from part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "88130f7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNet(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=7, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.2, inplace=False)\n",
      "    (3): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (4): ReLU()\n",
      "    (5): Dropout(p=0.2, inplace=False)\n",
      "    (6): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Dropout(p=0.2, inplace=False)\n",
      "    (9): Linear(in_features=128, out_features=2, bias=True)\n",
      "    (10): Tanh()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#Tuning Hyper Parameter-1 : Dropout\n",
    "#First using dropout rate of 0.2\n",
    "#Now we will be adding dropout layers\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, dropout_rate):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(7, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=dropout_rate),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=dropout_rate),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=dropout_rate),\n",
    "            nn.Linear(128, 2),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "\n",
    "# Create an instance of the neural network\n",
    "model = NeuralNet(dropout_rate = 0.2)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "cd65c58b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " For Epoch [10/100], training loss is: 0.628 and validation loss is: 0.575\n",
      " For Epoch [20/100], training loss is: 0.574 and validation loss is: 0.492\n",
      " For Epoch [30/100], training loss is: 0.545 and validation loss is: 0.491\n",
      " For Epoch [40/100], training loss is: 0.532 and validation loss is: 0.465\n",
      " For Epoch [50/100], training loss is: 0.509 and validation loss is: 0.443\n",
      " For Epoch [60/100], training loss is: 0.489 and validation loss is: 0.501\n",
      " For Epoch [70/100], training loss is: 0.496 and validation loss is: 0.444\n",
      " For Epoch [80/100], training loss is: 0.503 and validation loss is: 0.429\n",
      " For Epoch [90/100], training loss is: 0.490 and validation loss is: 0.484\n",
      " For Epoch [100/100], training loss is: 0.484 and validation loss is: 0.462\n",
      "Accuracy of the model on the test set is: 73.68%\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Define the batch size and create a DataLoader for the training set\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Create a DataLoader for the validation set\n",
    "val_loader = DataLoader(TensorDataset(X_val, y_val), batch_size=batch_size)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "# Train the model\n",
    "#Lets define this as our fit function\n",
    "def fit():\n",
    "    num_epochs = 100\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training\n",
    "        train_loss = 0\n",
    "        for (inputs, targets) in train_loader:\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_fn(outputs, targets)\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Calculate training loss\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        # Validation\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for (inputs, targets) in val_loader:\n",
    "                outputs = model(inputs)\n",
    "                loss = loss_fn(outputs, targets)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        # Print epoch losses for every 10 epochs\n",
    "        if (epoch+1)%10 == 0:\n",
    "            print(f\" For Epoch [{epoch+1}/{num_epochs}], training loss is: {train_loss/len(train_loader):.3f} and validation loss is: {val_loss/len(val_loader):.3f}\")\n",
    "fit()\n",
    "#Now make predictions on test set\n",
    "with torch.no_grad():\n",
    "    outputs = model(X_test)\n",
    "    predicted = torch.argmax(outputs, dim=1)\n",
    "    accuracy = (predicted == y_test).sum().item() / len(y_test)\n",
    "    print(f\"Accuracy of the model on the test set is: {accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90917c72",
   "metadata": {},
   "source": [
    "### Setup 9: Tuning Hyper Parameter-3 : Activation Function as Relu lets keep dropout at 0.2 and fix Adam Optimizer from part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "b0880f0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNet(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=7, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.2, inplace=False)\n",
      "    (3): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (4): ReLU()\n",
      "    (5): Dropout(p=0.2, inplace=False)\n",
      "    (6): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Dropout(p=0.2, inplace=False)\n",
      "    (9): Linear(in_features=128, out_features=2, bias=True)\n",
      "    (10): ReLU()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#Tuning Hyper Parameter-1 : Dropout\n",
    "#First using dropout rate of 0.2\n",
    "#Now we will be adding dropout layers\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, dropout_rate):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(7, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=dropout_rate),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=dropout_rate),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=dropout_rate),\n",
    "            nn.Linear(128, 2),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "\n",
    "# Create an instance of the neural network\n",
    "model = NeuralNet(dropout_rate = 0.2)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "b1b90694",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " For Epoch [10/100], training loss is: 0.460 and validation loss is: 0.483\n",
      " For Epoch [20/100], training loss is: 0.480 and validation loss is: 0.483\n",
      " For Epoch [30/100], training loss is: 0.453 and validation loss is: 0.444\n",
      " For Epoch [40/100], training loss is: 0.465 and validation loss is: 0.445\n",
      " For Epoch [50/100], training loss is: 0.466 and validation loss is: 0.465\n",
      " For Epoch [60/100], training loss is: 0.462 and validation loss is: 0.476\n",
      " For Epoch [70/100], training loss is: 0.462 and validation loss is: 0.469\n",
      " For Epoch [80/100], training loss is: 0.455 and validation loss is: 0.452\n",
      " For Epoch [90/100], training loss is: 0.459 and validation loss is: 0.461\n",
      " For Epoch [100/100], training loss is: 0.458 and validation loss is: 0.459\n",
      "Accuracy of the model on the test set is: 75.00%\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Define the batch size and create a DataLoader for the training set\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Create a DataLoader for the validation set\n",
    "val_loader = DataLoader(TensorDataset(X_val, y_val), batch_size=batch_size)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "# Train the model\n",
    "#Lets define this as our fit function\n",
    "def fit():\n",
    "    num_epochs = 100\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training\n",
    "        train_loss = 0\n",
    "        for (inputs, targets) in train_loader:\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_fn(outputs, targets)\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Calculate training loss\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        # Validation\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for (inputs, targets) in val_loader:\n",
    "                outputs = model(inputs)\n",
    "                loss = loss_fn(outputs, targets)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        # Print epoch losses for every 10 epochs\n",
    "        if (epoch+1)%10 == 0:\n",
    "            print(f\" For Epoch [{epoch+1}/{num_epochs}], training loss is: {train_loss/len(train_loader):.3f} and validation loss is: {val_loss/len(val_loader):.3f}\")\n",
    "fit()\n",
    "#Now make predictions on test set\n",
    "with torch.no_grad():\n",
    "    outputs = model(X_test)\n",
    "    predicted = torch.argmax(outputs, dim=1)\n",
    "    accuracy = (predicted == y_test).sum().item() / len(y_test)\n",
    "    print(f\"Accuracy of the model on the test set is: {accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ca96e5",
   "metadata": {},
   "source": [
    "### Base Model - Setup 1 Optimizer - Adam, Dropout - 0.2, Activation Function - Sigmoid, Accuracy 78.95"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8831546",
   "metadata": {},
   "source": [
    "### Applying Weight Decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "9efd6b42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNet(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=7, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.2, inplace=False)\n",
      "    (3): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (4): ReLU()\n",
      "    (5): Dropout(p=0.2, inplace=False)\n",
      "    (6): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Dropout(p=0.2, inplace=False)\n",
      "    (9): Linear(in_features=128, out_features=2, bias=True)\n",
      "    (10): Sigmoid()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#Now we will be adding dropout layers\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, dropout_rate):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(7, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=dropout_rate),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=dropout_rate),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=dropout_rate),\n",
    "            nn.Linear(128, 2),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "\n",
    "# Create an instance of the neural network\n",
    "model = NeuralNet(dropout_rate = 0.2)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "619a6ce8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " For Epoch [10/100], training loss is: 0.645 and validation loss is: 0.603\n",
      " For Epoch [20/100], training loss is: 0.600 and validation loss is: 0.553\n",
      " For Epoch [30/100], training loss is: 0.569 and validation loss is: 0.528\n",
      " For Epoch [40/100], training loss is: 0.554 and validation loss is: 0.523\n",
      " For Epoch [50/100], training loss is: 0.548 and validation loss is: 0.497\n",
      " For Epoch [60/100], training loss is: 0.544 and validation loss is: 0.521\n",
      " For Epoch [70/100], training loss is: 0.529 and validation loss is: 0.494\n",
      " For Epoch [80/100], training loss is: 0.535 and validation loss is: 0.494\n",
      " For Epoch [90/100], training loss is: 0.531 and validation loss is: 0.492\n",
      " For Epoch [100/100], training loss is: 0.524 and validation loss is: 0.504\n",
      "Accuracy of the model is: 72.37%\n"
     ]
    }
   ],
   "source": [
    "# Create an instance of the neural network\n",
    "model = NeuralNet(dropout_rate=0.2)\n",
    "\n",
    "# Define the batch size and create a DataLoader for the training set\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Create a DataLoader for the validation set\n",
    "val_loader = DataLoader(TensorDataset(X_val, y_val), batch_size=batch_size)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=0.0001) # set weight decay here\n",
    "\n",
    "# Train the model\n",
    "def fit():\n",
    "    num_epochs = 100\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training\n",
    "        train_loss = 0\n",
    "        for (inputs, targets) in train_loader:\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_fn(outputs, targets)\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Calculate training loss\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        # Validation\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for (inputs, targets) in val_loader:\n",
    "                outputs = model(inputs)\n",
    "                loss = loss_fn(outputs, targets)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        # Print epoch losses for every 10 epochs\n",
    "        if (epoch+1)%10 == 0:\n",
    "            print(f\" For Epoch [{epoch+1}/{num_epochs}], training loss is: {train_loss/len(train_loader):.3f} and validation loss is: {val_loss/len(val_loader):.3f}\")\n",
    "\n",
    "fit()\n",
    "#Now make predictions on test set\n",
    "with torch.no_grad():\n",
    "    outputs = model(X_test)\n",
    "    predicted = torch.argmax(outputs, dim=1)\n",
    "    accuracy = (predicted == y_test).sum().item() / len(y_test)\n",
    "    print(f\"Accuracy of the model is: {accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ddd4d9",
   "metadata": {},
   "source": [
    "### Applying Batch Normalization To Our Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "7fc17457",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNet(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=7, out_features=128, bias=True)\n",
      "    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.2, inplace=False)\n",
      "    (4): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (5): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): ReLU()\n",
      "    (7): Dropout(p=0.2, inplace=False)\n",
      "    (8): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (9): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (10): ReLU()\n",
      "    (11): Dropout(p=0.2, inplace=False)\n",
      "    (12): Linear(in_features=128, out_features=2, bias=True)\n",
      "    (13): Sigmoid()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, dropout_rate):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(7, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=dropout_rate),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=dropout_rate),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=dropout_rate),\n",
    "            nn.Linear(128, 2),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "# Create an instance of the neural network\n",
    "model = NeuralNet(dropout_rate = 0.2)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "4f411a59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " For Epoch [10/100], training loss is: 0.423 and validation loss is: 0.635\n",
      " For Epoch [20/100], training loss is: 0.420 and validation loss is: 0.660\n",
      " For Epoch [30/100], training loss is: 0.439 and validation loss is: 0.588\n",
      " For Epoch [40/100], training loss is: 0.428 and validation loss is: 0.598\n",
      " For Epoch [50/100], training loss is: 0.431 and validation loss is: 0.629\n",
      " For Epoch [60/100], training loss is: 0.425 and validation loss is: 0.614\n",
      " For Epoch [70/100], training loss is: 0.422 and validation loss is: 0.606\n",
      " For Epoch [80/100], training loss is: 0.416 and validation loss is: 0.617\n",
      " For Epoch [90/100], training loss is: 0.422 and validation loss is: 0.554\n",
      " For Epoch [100/100], training loss is: 0.433 and validation loss is: 0.658\n",
      "Accuracy of the model is: 71.05%\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Define the batch size and create a DataLoader for the training set\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Create a DataLoader for the validation set\n",
    "val_loader = DataLoader(TensorDataset(X_val, y_val), batch_size=batch_size)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "# Train the model\n",
    "#Lets define this as our fit function\n",
    "def fit():\n",
    "    num_epochs = 100\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training\n",
    "        train_loss = 0\n",
    "        for (inputs, targets) in train_loader:\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_fn(outputs, targets)\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Calculate training loss\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        # Validation\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for (inputs, targets) in val_loader:\n",
    "                outputs = model(inputs)\n",
    "                loss = loss_fn(outputs, targets)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        # Print epoch losses for every 10 epochs\n",
    "        if (epoch+1)%10 == 0:\n",
    "            print(f\" For Epoch [{epoch+1}/{num_epochs}], training loss is: {train_loss/len(train_loader):.3f} and validation loss is: {val_loss/len(val_loader):.3f}\")\n",
    "fit()\n",
    "#Now make predictions on test set\n",
    "with torch.no_grad():\n",
    "    outputs = model(X_test)\n",
    "    predicted = torch.argmax(outputs, dim=1)\n",
    "    accuracy = (predicted == y_test).sum().item() / len(y_test)\n",
    "    print(f\"Accuracy of the model is: {accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb2e5a6",
   "metadata": {},
   "source": [
    "### Apply Learn Rate Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "id": "fc0a20b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNet(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=7, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.2, inplace=False)\n",
      "    (3): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (4): ReLU()\n",
      "    (5): Dropout(p=0.2, inplace=False)\n",
      "    (6): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Dropout(p=0.2, inplace=False)\n",
      "    (9): Linear(in_features=128, out_features=2, bias=True)\n",
      "    (10): Sigmoid()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#Now we will be adding dropout layers\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, dropout_rate):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(7, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=dropout_rate),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=dropout_rate),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=dropout_rate),\n",
    "            nn.Linear(128, 2),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "\n",
    "# Create an instance of the neural network\n",
    "model = NeuralNet(dropout_rate = 0.2)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "id": "dbd4d0d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " For Epoch [10/100], training loss is: 0.521 and validation loss is: 0.498\n",
      "Epoch 00014: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00020: reducing learning rate of group 0 to 1.0000e-05.\n",
      " For Epoch [20/100], training loss is: 0.516 and validation loss is: 0.530\n",
      "Epoch 00026: reducing learning rate of group 0 to 1.0000e-06.\n",
      " For Epoch [30/100], training loss is: 0.510 and validation loss is: 0.531\n",
      "Epoch 00032: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 00038: reducing learning rate of group 0 to 1.0000e-08.\n",
      " For Epoch [40/100], training loss is: 0.519 and validation loss is: 0.497\n",
      " For Epoch [50/100], training loss is: 0.513 and validation loss is: 0.468\n",
      " For Epoch [60/100], training loss is: 0.516 and validation loss is: 0.493\n",
      " For Epoch [70/100], training loss is: 0.513 and validation loss is: 0.494\n",
      " For Epoch [80/100], training loss is: 0.512 and validation loss is: 0.508\n",
      " For Epoch [90/100], training loss is: 0.519 and validation loss is: 0.502\n",
      " For Epoch [100/100], training loss is: 0.520 and validation loss is: 0.522\n",
      "Accuracy of the model is: 76.32%\n"
     ]
    }
   ],
   "source": [
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Define the learning rate scheduler\n",
    "lr_scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5, verbose=True)\n",
    "\n",
    "# Train the model\n",
    "def fit():\n",
    "    num_epochs = 100\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training\n",
    "        train_loss = 0\n",
    "        for (inputs, targets) in train_loader:\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_fn(outputs, targets)\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Calculate training loss\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        # Validation\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for (inputs, targets) in val_loader:\n",
    "                outputs = model(inputs)\n",
    "                loss = loss_fn(outputs, targets)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        # Step the learning rate scheduler\n",
    "        lr_scheduler.step(val_loss)\n",
    "\n",
    "        # Print epoch losses for every 10 epochs\n",
    "        if (epoch+1)%10 == 0:\n",
    "            print(f\" For Epoch [{epoch+1}/{num_epochs}], training loss is: {train_loss/len(train_loader):.3f} and validation loss is: {val_loss/len(val_loader):.3f}\")\n",
    "fit()\n",
    "#Now make predictions on test set\n",
    "with torch.no_grad():\n",
    "    outputs = model(X_test)\n",
    "    predicted = torch.argmax(outputs, dim=1)\n",
    "    accuracy = (predicted == y_test).sum().item() / len(y_test)\n",
    "    print(f\"Accuracy of the model is: {accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bda5e2f",
   "metadata": {},
   "source": [
    "### Applying Gradient Clipping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "id": "69825573",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNet(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=7, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.2, inplace=False)\n",
      "    (3): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (4): ReLU()\n",
      "    (5): Dropout(p=0.2, inplace=False)\n",
      "    (6): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Dropout(p=0.2, inplace=False)\n",
      "    (9): Linear(in_features=128, out_features=2, bias=True)\n",
      "    (10): Sigmoid()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "id": "7a591dd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " For Epoch [10/100], training loss is: 0.645 and validation loss is: 0.604\n",
      " For Epoch [20/100], training loss is: 0.603 and validation loss is: 0.555\n",
      " For Epoch [30/100], training loss is: 0.581 and validation loss is: 0.522\n",
      " For Epoch [40/100], training loss is: 0.569 and validation loss is: 0.507\n",
      " For Epoch [50/100], training loss is: 0.556 and validation loss is: 0.496\n",
      " For Epoch [60/100], training loss is: 0.543 and validation loss is: 0.527\n",
      " For Epoch [70/100], training loss is: 0.530 and validation loss is: 0.505\n",
      " For Epoch [80/100], training loss is: 0.528 and validation loss is: 0.508\n",
      " For Epoch [90/100], training loss is: 0.525 and validation loss is: 0.505\n",
      " For Epoch [100/100], training loss is: 0.528 and validation loss is: 0.495\n",
      "Accuracy of the model is: 75.00%\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.utils as torch_utils\n",
    "# Define the loss function and optimizer\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Define the maximum gradient norm value\n",
    "max_grad_norm = 1.0\n",
    "\n",
    "# Train the model\n",
    "def fit():\n",
    "    num_epochs = 100\n",
    "    for epoch in range(num_epochs):\n",
    "        pass\n",
    "        # Training\n",
    "        train_loss = 0\n",
    "        for (inputs, targets) in train_loader:\n",
    "            pass\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_fn(outputs, targets)\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "\n",
    "            # Clip gradients to avoid exploding gradients\n",
    "            torch_utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            # Calculate training loss\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        # Validation\n",
    "        val_loss = 0\n",
    "        pass\n",
    "        with torch.no_grad():\n",
    "            for (inputs, targets) in val_loader:\n",
    "                outputs = model(inputs)\n",
    "                loss = loss_fn(outputs, targets)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        # Print epoch losses for every 10 epochs\n",
    "        if (epoch+1)%10 == 0:\n",
    "            pass\n",
    "            print(f\" For Epoch [{epoch+1}/{num_epochs}], training loss is: {train_loss/len(train_loader):.3f} and validation loss is: {val_loss/len(val_loader):.3f}\")\n",
    "\n",
    "fit()\n",
    "#Now make predictions on test set\n",
    "with torch.no_grad():\n",
    "    outputs = model(X_test)\n",
    "    predicted = torch.argmax(outputs, dim=1)\n",
    "    accuracy = (predicted == y_test).sum().item() / len(y_test)\n",
    "    print(f\"Accuracy of the model is: {accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c538c383",
   "metadata": {},
   "source": [
    "## Applying K-fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "id": "8915b4bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNet(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=7, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.2, inplace=False)\n",
      "    (3): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (4): ReLU()\n",
      "    (5): Dropout(p=0.2, inplace=False)\n",
      "    (6): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Dropout(p=0.2, inplace=False)\n",
      "    (9): Linear(in_features=128, out_features=2, bias=True)\n",
      "    (10): Sigmoid()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "id": "c8448c52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model is: 78.95%\n"
     ]
    }
   ],
   "source": [
    "# Define a function to train and evaluate the model using K-Fold Cross Validation\n",
    "def train_and_evaluate(X, y, k=5, dropout_rate=0.2, num_epochs=100, lr=0.001):\n",
    "    kfs = KFold(n_splits=k)\n",
    "    accuracies = []\n",
    "    fold_num = 1\n",
    "    \n",
    "    for train_idx, val_idx in kfs.split(X):\n",
    "        # Spliting  the data into training and validation sets\n",
    "        X_train, y_train = X[train_idx], y[train_idx]\n",
    "        X_val, y_val = X[val_idx], y[val_idx]\n",
    "\n",
    "        # Create a DataLoader for the training sets\n",
    "        train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=32, shuffle=True)\n",
    "\n",
    "        # Create a DataLoader for the validationsets\n",
    "        val_loader = DataLoader(TensorDataset(X_val, y_val), batch_size=32)\n",
    "\n",
    "        # Initialzing the model and the loss function\n",
    "        model = NeuralNet(dropout_rate)\n",
    "        loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "        # Initialize the optimizer\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "        # Train the model\n",
    "        for epoch in range(num_epochs):\n",
    "            # Training\n",
    "            pass\n",
    "            train_loss = 0\n",
    "            for (inputs, targets) in train_loader:\n",
    "                # Forward pass\n",
    "                outputs = model(inputs)\n",
    "                loss = loss_fn(outputs, targets)\n",
    "\n",
    "                # Backward pass and optimization\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                # Calculate training loss\n",
    "                train_loss += loss.item()\n",
    "\n",
    "            # Validation\n",
    "            val_loss = 0\n",
    "            pass\n",
    "            with torch.no_grad():\n",
    "            \n",
    "                for (inputs, targets) in val_loader:\n",
    "                    outputs = model(inputs)\n",
    "                    pass\n",
    "                    for i in val_loader:\n",
    "                        pass\n",
    "                    loss = loss_fn(outputs, targets)\n",
    "                    for j in val_loader:\n",
    "                        pass\n",
    "                    val_loss += loss.item()\n",
    "                    \n",
    "\n",
    "# Evaluate the model on the test set\n",
    "with torch.no_grad():\n",
    "    outputs = model(X_test)\n",
    "    pass\n",
    "    predicted = torch.argmax(outputs, dim=1)\n",
    "    accuracy = (predicted == y_test).sum().item() / len(y_test)\n",
    "    print(f\"Accuracy of the model is: {accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "id": "5275a41d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Data\n",
    "accuracies = [78.95, 72.37, 71.05, 76.32, 75.00, 78.95]\n",
    "\n",
    "# Define x-axis labels\n",
    "x_labels = ['Base Model', 'Weight Decay', 'Batch Normalization', 'Learn Rate Scheduler', 'Gradient Clipping', 'K-Fold']\n",
    "\n",
    "# Plot\n",
    "plt.plot(accuracies)\n",
    "\n",
    "# Customize plot\n",
    "plt.title(\"Accuracy Plot\",color = 'red')\n",
    "plt.xlabel(\"Model\",color = 'red')\n",
    "plt.ylabel(\"Accuracy (%)\",color = 'red')\n",
    "plt.xticks(range(len(accuracies)), x_labels, rotation=45, color='red')  # Set x-axis labels color to red\n",
    "plt.ylim([0, 100])\n",
    "plt.grid(True)\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
